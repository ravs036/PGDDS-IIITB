############################ Handwritten Digit Recogniser #################################
# 1. Business Understanding
# 2. Data Understanding
# 3. Data Preparation
# 4. Model Building 
#  4.1 Linear kernel
#  4.2 RBF Kernel
# 5. Hyperparameter tuning and cross validation
# 6. Results

#####################################################################################

# 1. Business Understanding: 

# A classic problem in the field of pattern recognition is that of handwritten digit recognition.
# Suppose that you have an image of a digit submitted by a user via a scanner, a tablet, or 
# other digital devices. The goal is to develop a model that can correctly identify the
# digit (between 0-9) written in an image.

#####################################################################################

# 2. Data Understanding: 

# The dataset contains grey scale pixel information having 784 pixels (28*28) and a 
# label column in the data which shows what kind of digit it is.
# Each row is the attribute of a digit 

# Train Number of Instances: 60,000
# Train Number of Attributes: 785

# Test Number of Instances: 10,000
# Test Number of Attributes: 785

#####################################################################################

#3. Data Preparation: 

##Loading Neccessary libraries

library(caret)
library(kernlab)
library(dplyr)
library(readr)
library(ggplot2)
library(gridExtra)

setwd("D:/PGDDS/SVM Digit Recognition/SVM Dataset")   # set as required

#Loading Data
mnist_train <- read.csv("mnist_train.csv",stringsAsFactors = FALSE,header = FALSE)
mnist_test <- read.csv("mnist_test.csv",stringsAsFactors = FALSE,header = FALSE)

#View loaded Data
View(mnist_train)
View(mnist_test)

# Renaming the column names as first coloumn seems to be the label
colnames(mnist_train)[1] <- "Digit"
colnames(mnist_test)[1] <- "Digit"

#Understanding Dimensions
dim(mnist_train)
dim(mnist_test)

#Structure of the dataset & checking for datatypes other than numeric
str(mnist_train)
mnist_train[, lapply(mnist_train[,-1], is.numeric) == FALSE]   # only numeric
str(mnist_test)
mnist_test[, lapply(mnist_test[,-1], is.numeric) == FALSE]     # same, only numeric

#Exploring the data and checking distribution level wise
summary(factor(mnist_train$Digit))   ### 10 classes in the data (0-9)
summary(factor(mnist_test$Digit))    ### 10 classes in the data (0-9)

# Duplicated rows
sum(duplicated(mnist_train)) # no duplicate rows
sum(duplicated(mnist_test)) # no duplicate rows

# Checking for NAs
sum(sapply(mnist_train, function(x) sum(is.na(x)))) # There are no missing values
sum(sapply(mnist_test, function(x) sum(is.na(x)))) # There are no missing values

# Changing output variable "Digit" to factor type 
mnist_train$Digit <- as.factor(mnist_train$Digit)
mnist_test$Digit <- as.factor(mnist_test$Digit)

# Extracting 15% records (9000 rows as train data) from mnist_train for comfortable computation
# test dataset remains same as provided

set.seed(100)
indices <- sample(1:nrow(mnist_train), 0.15*nrow(mnist_train))
train <- mnist_train[indices,]
test <- mnist_test

# Check for uniformity across levels and it holds goods for train and test dataset
ggplot(train,aes(x = factor(Digit),fill=factor(Digit)))+geom_bar()
ggplot(test,aes(x = factor(Digit),fill=factor(Digit)))+geom_bar() 


######################################################################################

# 4. Model Building
#    4.1 Linear Kernel

# With default parameters C = 1
Model1_linear <- ksvm(Digit ~., data = train, scaled = FALSE, kernel = "vanilladot", C=1)
Model1_linear

# Hyperparameter  : C = 1
# Support Vectors : 2545

# Confusion matrix - Linear Kernel- train dataset
Eval1_linear_train <- predict(Model1_linear, train)
confusionMatrix(Eval1_linear_train,train$Digit)   

# Train Accuracy : 1

# Confusion matrix - Linear Kernel - test dataset
Eval1_linear <- predict(Model1_linear, test)
confusionMatrix(Eval1_linear,test$Digit)

# Accuracy : 0.9182

#----- Hyperparameter Tuning using Cross Validation for Linear Kernal-----------#

grid.linear <- expand.grid(C= c(1e-08,1e-07,5e-07,1e-06,2e-06))

Linear.fit <- train(Digit~.,data=train,method="svmLinear",
                   metric = "Accuracy",tuneGrid=grid.linear,
                    trControl= trainControl(method = "cv", number = 3, verboseIter = TRUE))    # Warning message to be ignored

               # Warning() message ignored

Linear.fit     # Optimal C = 1e-06
plot(Linear.fit)

# Support Vector Machines with Linear Kernel 

# 9000 samples
# 784 predictor
# 10 classes: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' 

# No pre-processing
# Resampling: Cross-Validated (3 fold) 
# Summary of sample sizes: 5997, 6001, 6002 
# Resampling results across tuning parameters:
#  
#  C      Accuracy   Kappa    
# 1e-08  0.8778937  0.8642890
# 1e-07  0.9187813  0.9097402
# 5e-07  0.9267816  0.9186294
# 1e-06  0.9270035  0.9188754
# 2e-06  0.9248925  0.9165278

# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was C = 1e-06.

# Confusion matric - Linear.fit- test dataset
Eval_cv_linear <- predict(Linear.fit, test)
confusionMatrix(Eval_cv_linear, test$Digit)

# Accuracy : 0.93    # slight increase

# Even after crossvalidation tuning Linear Kernel is able to achieve accuracy of only 0.93.
# This suggest that linear kernel might not be the best algorithm for this data.
# Next step : to try RBF kernel

#---------------------------Using RBF Kernel-------------------------------------#

Model1_RBF <- ksvm(Digit~., data = train, scaled = FALSE, kernel = "rbfdot")
Model1_RBF

# Hyper Parameter :     C = 1
#                 : sigma = 1.63143391566721e-07 
# Support Vectors : 3543

# Confusion matrix - RBF Kernel- train dataset
Eval1_RBF_train<- predict(Model1_RBF, train)
confusionMatrix(Eval1_RBF_train,train$Digit)   

# Training accuracy : 0.9826

# Confusion matrix - RBF Kernel- test dataset 
Eval1_RBF<- predict(Model1_RBF, test)
confusionMatrix(Eval1_RBF,test$Digit)

# Accuracy : 0.9555


#######################################################################################

# 5. Hyperparameter tuning and cross validation

# Cross validation for RBF Model to find optimum value of sigma and C 
# Cross Validation folds = 3 to reduce computaion time
# Range of sigma = 1.63e-7, 2.63e-7, 3.63e-7, 4.63e-7,5.63e-7, c = 1:3


trainControl <- trainControl(method = "cv", number = 3, verboseIter = TRUE)

metric <- "Accuracy"     # we have set metric as accuracy

set.seed(100)

grid.RBF <- expand.grid(.sigma = c(1.63e-7, 2.63e-7, 3.63e-7, 4.63e-7,5.63e-7),.C=c(1,2,3))

RBF.fit <- train(Digit~.,data=train,method="svmRadial",
                                metric = metric,tuneGrid=grid.RBF,trControl=trainControl)
                                
            # Warning() message ignored           
RBF.fit
plot(RBF.fit)

# Output RBF.fit
# Support Vector Machines with Radial Basis Function Kernel 
 
# 9000 samples
# 784 predictor
# 10 classes: '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' 

# No pre-processing
# Resampling: Cross-Validated (3 fold) 
# Summary of sample sizes: 6001, 5999, 6000 
# Resampling results across tuning parameters:
  
# sigma     C  Accuracy   Kappa    
# 1.63e-07  1  0.9526666  0.9473984
# 1.63e-07  2  0.9583327  0.9536954
# 1.63e-07  3  0.9613324  0.9570289
# 2.63e-07  1  0.9591108  0.9545604
# 2.63e-07  2  0.9639996  0.9599929
# 2.63e-07  3  0.9648880  0.9609801
# 3.63e-07  1  0.9623330  0.9581412
# 3.63e-07  2  0.9662219  0.9624628
# 3.63e-07  3  0.9662217  0.9624625
# 4.63e-07  1  0.9637778  0.9597469
# 4.63e-07  2  0.9665553  0.9628333
# 4.63e-07  3  0.9665554  0.9628334
# 5.63e-07  1  0.9648890  0.9609817
# 5.63e-07  2  0.9672223  0.9635746
# 5.63e-07  3  0.9671111  0.9634511

# Accuracy was used to select the optimal model using the largest value.
# The final values used for the model were sigma = 5.63e-07 and C = 2.


# Confusion matrix - RBF.fit- test dataset
Eval_RBF.fit <- predict(RBF.fit,test)
confusionMatrix(Eval_RBF.fit,test$Digit)

# Accuracy     = 0.9678

################################################################################

# 6. Results

# FINAL MODEL is "RBF.fit" with tuned Hyperparameters
#                               C = 2, sigma = 5.63e-7, Support vectors  : 5061

# Accuracy of almost 97% with no overfitting 
# Sensitivities > 98.5 and Specificities > 99.6.

# I have also observed that when we take more data (i.e 33 %) in training data the 
# accuracy tends to increase towards 98 %. However, for easier computation i have taken 15%. 
